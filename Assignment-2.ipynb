{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c08ebab-396a-4d4f-9af9-92d9513170e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (1.7.2)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (3.0.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (3.9.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (5.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: click in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn PyPDF2 nltk sentence-transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0105eb12-6b7b-41f6-b492-8eebcd58d529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (2.20.1)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (75.8.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.75.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a25853-d5a0-4d75-bb58-6c45ed71f569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (1.26.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aadc773-0cb8-412a-aa4c-e31953617780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tika in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (3.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tika) (75.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from tika) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests->tika) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests->tika) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests->tika) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from requests->tika) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035a5b69-cac4-41a4-9bde-3e62714bb86b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytesseract in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (1.17.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (from pytesseract) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "231d4290-0f4c-48c1-a9d2-4b388df1ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\vamshi krishna babu\\appdata\\roaming\\python\\python313\\site-packages (21.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f081ba-1366-41bd-b319-ffeb2d3c7d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VAMSHI KRISHNA BABU\\AppData\\Roaming\\Python\\Python313\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 13:55:47,772 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tika server is starting/running. ✅\n",
      "Starting recursive search for PDF files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 14it [00:14,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: library error: FT_New_Memory_Face(Times-Bold): unknown file format\n",
      "\n",
      "MuPDF error: library error: FT_New_Memory_Face(Times-Bold): unknown file format\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 29it [00:30,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: PyMuPDF returned no text for Multi-attribute decision model.pdf. Trying Tika...\n",
      "INFO: Tika also returned no text for Multi-attribute decision model.pdf. Trying OCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 30it [00:50,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR done: Successfully extracted text from Multi-attribute decision model.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 43it [01:06,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: PyMuPDF returned no text for Case Generation.pdf. Trying Tika...\n",
      "SUCCESS: Completed with Tika: Case Generation.pdf\n",
      "INFO: PyMuPDF returned no text for Density Based Multiscale data condensation.pdf. Trying Tika...\n",
      "SUCCESS: Completed with Tika: Density Based Multiscale data condensation.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 45it [01:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: PyMuPDF returned no text for OSAVA_An Android App for Teaching a Course on Operating Systems.pdf. Trying Tika...\n",
      "SUCCESS: Completed with Tika: OSAVA_An Android App for Teaching a Course on Operating Systems.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 62it [01:27,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: PyMuPDF returned no text for On rule pruning using fuzzy NN.pdf. Trying Tika...\n",
      "SUCCESS: Completed with Tika: On rule pruning using fuzzy NN.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 71it [01:36,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: PyMuPDF returned no text for Combining Global and Local Surrogate Models.pdf. Trying Tika...\n",
      "INFO: Tika also returned no text for Combining Global and Local Surrogate Models.pdf. Trying OCR...\n",
      "OCR done: Successfully extracted text from Combining Global and Local Surrogate Models.pdf\n",
      "INFO: PyMuPDF returned no text for Meta Lamarckian.pdf. Trying Tika...\n",
      "INFO: Tika also returned no text for Meta Lamarckian.pdf. Trying OCR...\n",
      "OCR done: Successfully extracted text from Meta Lamarckian.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 72it [02:53,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully built database with 637 papers.\n",
      "Failed to process 0 papers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>paper</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amit Saxena</td>\n",
       "      <td>A Review of Clustering Techniques.pdf</td>\n",
       "      <td>review clustering techniques developments amit...</td>\n",
       "      <td>C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amit Saxena</td>\n",
       "      <td>Cardioprotection from ischemia.pdf</td>\n",
       "      <td>molecular cellular biochemistry kluwer academi...</td>\n",
       "      <td>C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amit Saxena</td>\n",
       "      <td>Controlled synthesis.pdf</td>\n",
       "      <td>advances natural sciences nanoscience nanotech...</td>\n",
       "      <td>C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amit Saxena</td>\n",
       "      <td>Cutting Edge.pdf</td>\n",
       "      <td>see discussions stats author profiles publicat...</td>\n",
       "      <td>C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amit Saxena</td>\n",
       "      <td>Endogenous IRAK-M Attenuates.pdf</td>\n",
       "      <td>issue necrosis triggers intense inflammatory r...</td>\n",
       "      <td>C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author                                  paper  \\\n",
       "0  Amit Saxena  A Review of Clustering Techniques.pdf   \n",
       "1  Amit Saxena     Cardioprotection from ischemia.pdf   \n",
       "2  Amit Saxena               Controlled synthesis.pdf   \n",
       "3  Amit Saxena                       Cutting Edge.pdf   \n",
       "4  Amit Saxena       Endogenous IRAK-M Attenuates.pdf   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  review clustering techniques developments amit...   \n",
       "1  molecular cellular biochemistry kluwer academi...   \n",
       "2  advances natural sciences nanoscience nanotech...   \n",
       "3  see discussions stats author profiles publicat...   \n",
       "4  issue necrosis triggers intense inflammatory r...   \n",
       "\n",
       "                                           full_path  \n",
       "0  C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...  \n",
       "1  C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...  \n",
       "2  C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...  \n",
       "3  C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...  \n",
       "4  C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Da...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from tika import parser as tika_parser\n",
    "from tika import detector\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "\n",
    "\n",
    "tesseract_install_path = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_install_path\n",
    "\n",
    "try:\n",
    "    detector.from_buffer(None)\n",
    "    print(\"Tika server is starting/running. ✅\")\n",
    "except Exception as e:\n",
    "    print(f\"--- TIKA ERROR ---\")\n",
    "    print(f\"Could not start Tika. Did you install Java 11+? Error: {e}\")\n",
    "\n",
    "# --- (Your poppler path variable) ---\n",
    "poppler_install_path = r\"C:\\Users\\VAMSHI KRISHNA BABU\\poppler\\poppler-25.07.0\\Library\\bin\"\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text using a 3-stage robust fallback method:\n",
    "    1. PyMuPDF (Fast) -> 2. Tika (Robust) -> 3. OCR (Image-based)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Method 1: Try PyMuPDF (fitz) ---\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "        if text.strip():\n",
    "            return text\n",
    "        print(f\"INFO: PyMuPDF returned no text for {os.path.basename(pdf_path)}. Trying Tika...\")\n",
    "    except Exception as e_fitz:\n",
    "        if \"no such file\" in str(e_fitz):\n",
    "             print(f\"ERROR: File path is too long. Please rename: {pdf_path}\")\n",
    "             return \"\"\n",
    "        print(f\"INFO: PyMuPDF failed on {os.path.basename(pdf_path)} ({e_fitz}). Trying Tika fallback...\")\n",
    "\n",
    "    # --- Method 2: Fallback to Tika ---\n",
    "    try:\n",
    "        parsed = tika_parser.from_file(pdf_path)\n",
    "        text = parsed.get('content')\n",
    "        if text and text.strip():\n",
    "            print(f\"SUCCESS: Completed with Tika: {os.path.basename(pdf_path)}\")\n",
    "            return text.strip()\n",
    "        print(f\"INFO: Tika also returned no text for {os.path.basename(pdf_path)}. Trying OCR...\")\n",
    "    except Exception as e_tika:\n",
    "        print(f\"ERROR: Tika failed on {os.path.basename(pdf_path)} ({e_tika}). Trying OCR fallback...\")\n",
    "\n",
    "    # --- Method 3: Fallback to OCR (Tesseract) ---\n",
    "    try:\n",
    "        # We use the poppler path we defined earlier\n",
    "        images = convert_from_path(pdf_path, poppler_path=poppler_install_path) \n",
    "        \n",
    "        ocr_text = \"\"\n",
    "        for img in images:\n",
    "            # Now pytesseract knows where to find the 'tesseract.exe'\n",
    "            ocr_text += pytesseract.image_to_string(img, lang='eng')\n",
    "        \n",
    "        if ocr_text.strip():\n",
    "            print(f\"OCR done: Successfully extracted text from {os.path.basename(pdf_path)}\")\n",
    "            return ocr_text\n",
    "        \n",
    "        print(f\"WARN: All 3 methods failed to find text in {os.path.basename(pdf_path)}. Skipping.\")\n",
    "        return \"\"\n",
    "                \n",
    "    except Exception as e_ocr:\n",
    "        # This will now only show if poppler is wrong, or Tesseract has another issue\n",
    "        print(f\"ERROR: OCR failed for {os.path.basename(pdf_path)} ({e_ocr}). Skipping.\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and tokenizes text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# --- (Keep all your imports and helper functions the same) ---\n",
    "# (e.g., extract_text_from_pdf, preprocess_text, tesseract_cmd, etc.)\n",
    "\n",
    "\n",
    "# --- Copy all your functions from Phase 1 here ---\n",
    "# (imports, tesseract_cmd, poppler_path, extract_text_from_pdf, preprocess_text, etc.)\n",
    "\n",
    "def build_author_database(dataset_path):\n",
    "    \"\"\"\n",
    "    Builds the database by searching RECURSIVELY for all PDFs\n",
    "    and returns a list of files that failed to process.\n",
    "    \"\"\"\n",
    "    authors_data = []\n",
    "    failed_files = []\n",
    "    \n",
    "    print(\"Starting recursive search for PDF files...\")\n",
    "    \n",
    "    for root, dirs, files in tqdm(os.walk(dataset_path), desc=\"Scanning Folders\"):\n",
    "        for paper_file in files:\n",
    "            if paper_file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, paper_file)\n",
    "                relative_path = os.path.relpath(root, dataset_path)\n",
    "                author_name = relative_path.split(os.path.sep)[0]\n",
    "                \n",
    "                raw_text = extract_text_from_pdf(pdf_path)\n",
    "                \n",
    "                if raw_text:\n",
    "                    processed_text = preprocess_text(raw_text)\n",
    "                    authors_data.append({\n",
    "                        'author': author_name,\n",
    "                        'paper': paper_file,\n",
    "                        'processed_text': processed_text,\n",
    "                        'full_path': pdf_path  # <-- THIS IS THE NEWLY ADDED LINE\n",
    "                    })\n",
    "                else:\n",
    "                    failed_files.append(pdf_path)\n",
    "                        \n",
    "    return pd.DataFrame(authors_data), failed_files\n",
    "\n",
    "# --- RE-RUN THE DATABASE BUILDER ---\n",
    "DATASET_FOLDER = r\"C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Dataset\"\n",
    "author_df, failed_papers = build_author_database(DATASET_FOLDER)\n",
    "\n",
    "print(f\"\\nSuccessfully built database with {len(author_df)} papers.\")\n",
    "print(f\"Failed to process {len(failed_papers)} papers.\")\n",
    "author_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aeda396-5422-4741-bbf8-66a84d38a757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296cc218-c859-42cd-b6b2-cecfd39abd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Geeta Rani'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df['author'][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e72d54ed-8354-4b8a-a491-e000560abda4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'review clustering techniques developments amit saxena mukesh prasad akshansh gupta neha bharill om prakash patel aruna tiwari meng joo er weiping ding chinteng lin department computer science guru ghasidas vishwavidyalaya bilaspur india centre artificial intelligence university technology sydney sydney australia school computational integrative sciences jawaharlal nehru university new delhi india department computer science engineering indian institute technology indore india school electrical electronic engineering nanyang technological university singapore school computer technology nantong university nantong china abstract paper presents comprehensive study clustering exiting methods developments made various times clustering defined unsupervised learning objects grouped basis similarity inherent among different methods clustering objects hierarchical partitional grid density based model based approaches used methods discussed respective states art applicability measures similarity well evaluation criteria central components clustering also presented paper applications clustering fields like image segmentation object character recognition data mining highlighted keywords unsupervised learning clustering data mining pattern recognition similarity measures introduction grouping objects required various purposes different areas engineering science technology humanities medical science daily life take instance people suffering particular disease symptoms common placed group tagged label usually name disease evidently people possessing symptoms hence disease placed group patients grouped disease treated accordingly patients belonging group handled differently therefore essential medical expert diagnose symptoms patient correctly heshe placed wrong group whenever find labeled object place group label rather trivial task labels given advance however many occasions labeling information provided advance group objects basis similarity instances represent wide range problems occurring analysis data generic terms cases dealt scope classification precisely first case class label object given advance termed supervised classification whereas case class label tagged object advance termed unsupervised classification tremendous amount work supervised classification evidently reported literature widely main purpose behind study classification develop tool algorithm used predict class unknown object labeled tool algorithm called classifier objects classification process commonly represented instances patterns pattern consists number features also called attributes classification accuracy classifier judged fact many testing patterns classified correctly rich amount work supervised classification pioneer supervised classification algorithms found neural networks fuzzy sets pso rough sets decision tree bayes classifiers etc contrary supervised classification given labeled patterns unsupervised classification differs manner label assigned pattern unsupervised classification commonly known clustering learning operation central process classification supervised unsupervised used paper interchangeably spirit clustering essential component various data analysis machine learning based applications like regression prediction data mining etc according rokach clustering divides data patterns subsets way similar patterns clustered together patterns thereby managed wellformed evaluation designates population sampled formally conventionally clustering structure represented set subsets sk k means obviously instance sk belongs exactly one subset belong subset clustering objects also applicable charactering key features people recognizing basis similarity general may divide people different clusters basis gender height weight color vocal physical appearances hence clustering embraces several interdisciplinary areas mathematics statistics biology genetics use various terminology explain topologies formed using clustering analysis technique example biological taxonomies medical syndromes genetic genotypes manufacturing group technology topics identical problem create groups instances assign instance appropriate groups clustering considered difficult supervised classification label attached patterns clustering given label case supervised classification becomes clue grouping data objects whole whereas case clustering becomes difficult decide group pattern belong absence label several parameters features could considered fit clustering curse dimensionality add crisis high dimensionality leads high computational cost also affects consistency algorithms although feature selection methods reported solution sizes databases eg small large large also guide clustering criteria jain illustrated main aim data clustering search real groupings set instances points objects webster merriamwebster online dictionary explains clustering statistical classification method finding whether patterns comes various groups making quantitative comparisons different features evident discussion similarity central factor cluster hence clustering process natural grouping data based inherent similarity discovered clustering cases number clusters formed specified user numeric type data available represent features patterns group way extract information pertaining relationship among patterns make use numeric arithmetic features objects represented numeric values common approach define similarity taken measure distance among patterns lower distance eg euclidean distance two objects higher similarity vice versa overall paper organized follows various clustering techniques discussed section section presents measures similarity differentiating patterns section variants clustering methods presented evaluation criteria clustering techniques applied different problems provided section section highlights emerging applications clustering section describes clustering method select different applications followed conclusions section due wide range topics subject omission unbalancing certain topics presented paper denied objective paper however present comprehensive timeline study clustering concepts comparisons existing techniques important applications clustering techniques section discuss various clustering approaches inherent techniques reason different clustering approaches towards various techniques due fact precise definition notion cluster different clustering approaches proposed uses different inclusion principle fraley raftery suggested dividing clustering approaches two different groups hierarchical partitioning techniques han kamber suggested following three additional categories applying clustering techniques densitybased methods modelbased methods gridbased methods alternative categorization based induction principle different clustering approaches presented castro et al however number clusters available dataset divided decided users judiciously using approaches including heuristic trial error evolutionary user decides suitable number accuracy judged intracluster distance high otherwise accuracy become low fig shows taxonomy clustering approaches fig taxonomy clustering approaches hierarchical clustering hc methods hierarchical clustering methods clusters formed iteratively dividing patterns using topdown bottom approach two forms hierarchical method namely agglomerative divisive hierarchical clustering agglomerative follows bottomup approach builds clusters starting single object merging atomic clusters larger larger clusters objects finally lying single cluster otherwise certain termination conditions satisfied divisive hierarchical clustering follows topdown approach breaks cluster containing objects smaller clusters object forms cluster satisfies certain termination conditions hierarchical methods usually lead formation dendrograms shown fig fig hierarchical clustering dendrogram hierarchical clustering methods could grouped three categories based similarity measures linkages summarized following sections singlelinkage clustering type clustering often called connectedness minimum method nearest neighbour method singlelinkage clustering link two clusters made single element pair namely two elements one cluster closest clustering distance two clusters determined nearest distance member one cluster member cluster also defines similarity data equipped similarities similarity pair clusters considered equal greatest similarity member one cluster member cluster fig shows mapping single linkage clustering criteria two sets clusters b follow min b b b fig mapping single linkage clustering completelinkage clustering completelinkage clustering also called diameter maximum method furthest neighbour method distance two clusters determined longest distance member one cluster member cluster fig shows mapping complete linkage clustering criteria two sets clusters b follow max b b b fig mapping complete linkage clustering averagelinkage clustering average linkage clustering also known minimum variance method distance two clusters determined average distance member one cluster member cluster fig shows mapping average linkage clustering criteria two sets clusters b follow b b b b fig mapping average linkage clustering steps agglomerative divisive clustering steps agglomerative clustering ii steps divisive clustering make point separate cluster clustering satisfactory merge two clusters smallest intercluster distance end construct single cluster containing points clustering satisfactory split cluster yields two components largest intercluster distance end common criticism classical hc algorithms lack robustness hence sensitive noise outliers object assigned cluster considered means hc algorithms capable correcting possible previous misclassification computational complexity hc algorithms least high cost limits application largescale data sets disadvantages hc include tendency form spherical shapes reversal phenomenon normal hierarchical structure distorted requirement largescale datasets recent years hc algorithms also enriched new techniques modifications classical hc methods presented following section enhanced hierarchical clustering main deficiency hierarchical clustering two points clusters linked move clusters hierarchy algorithms use hierarchical clustering enhancements given balanced iterative reducing clustering using hierarchies birch birch contains idea cluster features cf cf triple n ls ss n number data objects cluster ls linear sum attribute values objects cluster ss sum squares attribute values objects cluster stored cftree form need keep tuples clusters main memory tuples main motivations birch lie two aspects ability deal large data sets robustness outliers also birch achieve comutational complexity ii clustering using representatives cure cure clustering technique dealing largescale databases robust towards outliers accepts clusters various shapes sizes performance good data sets birch cure handle outliers well cure clustering quality better birch reverse terms time complexity birch better cure attains computational complexity compared cure onlogn iii rock rock applied categorical data sets follows agglomerative hierarchical clustering algorithm based number links two records links capture number records similar algorithm use distance function cure also proposed rock uses random sample strategy handle large datasets iv chameleon chameleon hierarchical clustering algorithm clusters merged interconnectivity closeness proximity two clusters high relative internal interconnectivity clusters closeness items within clusters one limitation chameleon known low dimensional spaces applied high dimensions table features hierarchical clusteringbased enhanced methods name type data complexity ability handle high dimensional data birch numerical cure numerical onlogn yes rock categorical onnmmmanlogn chemeleon numerical categorical onm nlogn mlogn mm maximum number neighbours point average number neighbours point number initial subclusters produced graph partitioning algorithm partition clustering methods partitional clustering opposite hierarchical clustering data assigned k clusters without hierarchical structure optimizing criterion function commonly used criterion euclidean distance finds minimum distance points available clusters assigning point cluster algorithms studied category include kmeans pam clara clarans fuzzy cmeans dbscan etc fig shows partitional clustering approach data points partitional clusters fig partitional clustering approaches kmeans clustering kmeans algorithm one bestknown bench marked simplest clustering algorithms mostly applied solve clustering problems procedure given data set classified user defined number clusters k main idea define k centroids one cluster objective function j given follows minimize k n j j j j x c j j x c chosen distance measure data point j ix cluster centre jc fig shows flow diagram kmeans algorithm algorithm similar kmeans known lindebuzogray lbg algorithm suggested vector quantization vq signal compression context prototype vectors called code words constitute code book vq aims represent data reduced number elements minimizing information loss although k means clustering still one popular clustering algorithms yet limitation associated k means clustering include efficient universal method identifying initial partitions number clusters k b kmeans sensitive outliers noise even object quite far away cluster centroid still forced cluster thus distorts cluster shapes fig flow diagram k means algorithm procedure kmeans algorithm composed following steps initialization suppose decide form k clusters given dataset take k distinct points patterns randomly points represent initial group centroids centroids changing iteration clusters fixed need spend time decision choosing centroids assign object group closest centroid objects assigned recalculate positions k centroids repeat steps centroids longer move produces separation objects groups metric minimized calculated fuzzy cmeans clustering fuzzy cmeans fcm clustering method allows one point belong two clusters unlike kmeans one cluster assigned point method developed dunn improved bezdek procedure fuzzy cmeans similar kmeans based minimization following objective function n c ij j j j u x v fuzzy partition matrix exponent controlling degree fuzzy overlap fuzzy overlap refers fuzzy boundaries clusters number data points significant membership one cluster uij degree membership xi cluster j xi ith pattern ddimension data vj jth cluster center ddimension norm expressing similarity measured data center procedure fcm fcm suffers initial partition dependence well noise outliers like kmeans yager filev proposed mountain method estimate cluster centers initial partition gath geva addressed initialization problem dynamically adding cluster prototypes located space represented well previously generated centers set value c number cluster select initial cluster prototype c v v v x n compute distance j x v objects prototypes computer elements fuzzy partition matrix n j c c j ij l l x v u x v compute cluster prototypes j c n ij j n ij u x v u stop convergence attained number iterations exceeds given limit otherwise go step changing proximity distance improve performance fcm relation outliers another approach reducing effect noise outliers keller interpreted memberships compatibility points class prototype rather degree membership relaxes uij uij results possibilistic kmeans clustering algorithm conditions possibilistic fuzzy partition matrix ij u n j c j ij u n ij u n j c table features partition clustering based techniques name type data complexity ability handle high dimensional data kmean numerical pam numerical oknk clara numerical okkknk clarans numerical okn fuzzy cmeans numerical n number points dataset k number clusters defined kmeans algorithms problems like defining number clusters initially susceptibility local optima sensitivity outliers memory space unknown number iteration steps required cluster fuzzy c means clustering really suitable handling issues related understand ability patterns incompletenoisy data mixed media information human interaction provide approximate solutions faster mainly used discovering association rules functional dependencies well image retrieval however time complexity k means much less fcm thus k means works faster fcm advantages partition based algorithms includes relatively scalable simple ii suitable datasets compact spherical clusters wellseparated however disadvantages algorithms include poor cluster descriptors ii reliance user specify number clusters advance iii high sensitivity initialization phase noise outliers iv inability deal nonconvex clusters varying size density measures similarities similarity objects within cluster plays important role clustering process good cluster finds maximum similarity among objects measure similarity cluster mainly decided distance among members conventional cluster nonfuzzy member either belongs cluster wholly many clustering methods use distance measures determine similarity dissimilarity pair objects useful denote distance two instances xi xj dxi xj valid distance measure symmetric ie dxi xj dxj xi obtain minimum value ideally zero case identical vectors distance measure called metric distance measure also satisfies following properties triangle inequality k j j k j k x x x x x x x x x j j j x x x x x x minkowski distance measures numeric attributes measurement distance fundamental operation unsupervised learning process smaller distance two objects closer objects assumed basis similarity family distance measures minkowski metrics distance measured following equation r k r jk ik r x x ij xik value kth variable entity xjk value kth variable entity j popular common distance measure euclidean l norm r details unsupervised classification various noneuclidean distances seen saxena et al cosine measure cosine measure popular similarity score text mining information retrieval normalized inner product cosine measure defined j j j x x x x x x pearson correlation measure correlation coefficient first discovered bravais later shown person normalized pearson correlation two vectors xi xj defined j j j j j x x x x x x x x x x ix denotes average feature value x dimensions extended jaccard measure strehl ghosh represented extended jaccard measure follows j j j j x x x x x x x x dice coefficient measure independently developed thorvald srensen raymond dice dice coefficient measure similar extended jaccard measure defined j j j x x x x x x choice suitable similarity measure measures similarities applied millions applications clustering fact every clustering problem applies one similarity measures euclidean distance mostly applied find similarity two objects expressed numerically euclidean distance highly sensitive noise usually applied data hundreds attributes also features high values tend dominate others may applied translations nonnumeric objects numeric values almost nil minimum jaccard similarity coefficient suitable sufficiently employed documents word similarity measurement efficiency measurement program performance deal appropriately high stability failure mistake spelling occurred nevertheless method able detect overtype words data sets pearson correlation usually unable detect difference two variables cosine similarity also good choice document clustering invariant rotation linear transformations variants clustering methods graph theoretic clustering graph theoretic clustering method represents clusters via graphs edges graph connect instances represented nodes wellknown graphtheoretic algorithm based minimal spanning tree mst inconsistent edges edges whose weight case clustering length signicantly larger average nearby edge lengths another graph theoretic approach constructs graphs based limited neighbourhood sets graph theoretic clustering convenient represent clusters via graphs weak handling outliers especially mst well detecting overlapping clusters graph clustering involves task dividing nodes clusters edge density higher within clusters opposed across clusters natural classic popular statistical setting evaluating solutions problem stochastic block model also referred planted partition model general graph lpartition problem partition nodes undirected graph l equalsized groups minimize total number edges cross groups condon presented simple lineartime algorithm graph lpartition problem analyzed random planted lpartition model model n nodes graph partitioned l groups size nl two nodes group connected edge probability p two nodes different groups connected edge probability rp showed prn constant algorithm finds optimal partition probability expn graph clustering decomposes network sub networks based topological properties general look dense sub networks shown fig fig subnetwork clustering graph spectral clustering proposed donath hoffman emerging technique graph clustering consists algorithms cluster points using eigenvectors matrices derived data machine learning community spectral clustering made popular works shi malik useful tutorial available spectral clustering luxburg success spectral clustering mainly based fact make strong assumptions form clusters opposed kmeans resulting clusters form convex sets precise lie disjoint convex sets underlying space spectral clustering solve general problems like intertwined spirals moreover spectral clustering implemented efficiently even large data sets long make sure similarity graph sparse similarity graph chosen solve linear problem issues getting stuck local minima restarting algorithm several times different initializations however already mentioned choosing good similarity graph trivial spectral clustering quite unstable different choices parameters neighborhood graphs spectral clustering serve black box algorithm automatically detects correct clusters given data set considered powerful tool produce good results applied care literature partially graph spectral clustering seen spectral clustering algorithms would like state common spectral clustering algorithms assume data consists n points x xn arbitrary objects measure pair wise similarities sij sxi xj similarity function symmetric nonnegative denote corresponding similarity matrix sij j n unnormalized spectral clustering normalized spectral clustering according shi malik input similarity matrix r nn number k clusters construct construct similarity graph one ways described section let w weighted adjacency matrix compute unnormalized laplacian l compute first k eigenvectors u uk l let u r nk matrix containing vectors u uk columns n let yi r k vector corresponding ith row u cluster points yiin r k kmeans algorithm clusters c ck output clusters ak ai j yj ci input similarity matrix r nn number k clusters construct construct similarity graph one ways described section let w weighted adjacency matrix compute unnormalized laplacian l compute first k generalized eigenvectors u uk generalized eigen problem lu du let u r nk matrix containing vectors u uk columns n let yi r k vector corresponding ith row u cluster points yiin r k kmeans algorithm clusters c ck output clusters ak ai j yj ci model based clustering methods model based clustering methods optimize well find suitability given data mathematical models similar conventional clustering modelbased clustering methods also detect feature details cluster cluster represents concept class decision trees neural networks two frequently used induction methods decision trees representation data decision tree modelled hierarchical tree leaf denotes concept implies probabilistic description concept many algorithms produce classication trees defining unlabelled data number algorithms proposed conceptual clustering follows cluster michalski stepp cobweb fisher cyrus kolodner galois carpineto romano gcf talavera bjar inc hadzikadic yun iterate biswas weinberg fisher labyrinth thompson langley subdue jonyer cook holder unimem lebowitz witt hanson bauer cobweb one best known algorithms concept defines set objects object defined binary values property list aim achieve high predictability nominal variable values given cluster algorithm suitable clustering large database data ii neural networks neural networks represent cluster neuron whereas input data also represented neurons connected prototype neurons connection attributed weight initialized randomly learning weights adaptively popular neural algorithm clustering selforganizing map som som commonly used vector quantization feature extraction data visualization along clustering analysis algorithm constructs singlelayered network shown fig learning process takes place winnertakesall fashion prototype neurons compete current instance winner neuron whose weight vector closest instance currently presented winner neighbours learn weights adjusted sofms merits input space density approximation independence order input patterns number user dependent parameters cause problems applied real practice like kmeans algorithmsofm need predefine size lattice ie number clusters unknown circumstances additionally trained sofm may suffering input space density mis representation areas low pattern density may represented areas high density represented fig model single layered network mixture densitybased clustering xu wunsch described clustering perspective probability data objects drawn specic probability distribution overall distribution data assumed mixture several distributions data points derived different types density functions eg multivariate gaussian tdistribution families different parameters aim methods identify clusters distribution cheeseman stutz introduced algorithm named autoclass widely used covers broad variety distributions including gaussian bernoulli poisson lognormal distributions ester et al demonstrated algorithm called dbscan densitybased spatial clustering applications noise discovers clusters arbitrary shapes efficient large spatial databases wellknown densitybased techniques snob proposed wallace dowe mclust introduced fraley raftery among methods expectationmaximization em algorithm popular em algorithm log likelihood function maximize follows ln ln p x p x x denotes set observed data n x x x denotes set latent variables n complete data set formed x x joint distribution p x ruled set parameters major disadvantages em algorithm sensitivity selection initial parameters effect singular covariance matrix possibility convergence local optimum slow convergence rate procedure em algorithm gridbased clustering methods methods partition space nite number cells form grid structure operations clustering performed main advantage approach fast processing time need distance computations easy determine clusters neighbouring basic steps grid based algorithm many others interesting grid based techniques including sting statistical information grid approach wang yang muntz one highly scalable algorithm ability decompose data set various levels detail sting retrieves spatial data divides rectangular cells corresponding different levels resolution shown fig initialize parameters old e step evaluate old p x step reestimate parameters argmax new l check convergence convergence criterion satisfied let old new return step define set grid cells assign objects appropriate grid cell compute density cell eliminate cells whose density certain threshold form clusters contiguous groups dense cells fig rectangular cells corresponding different levels resolution cell higher level partitioned number smaller cells next lower level mean variance minimum maximum cell computed using normal uniform distribution statistical information cell calculated stored advance uses top approach answer spatial data queries wave cluster introduced sheikholeslami et al uses multiresolution approach like sting allows natural clustering become distinguishable uses signal processing technique decomposes signal different frequency subband data transformed preserve relative distance objects different levels resolution highly scalable handle outliers well suitable high dimensional data set considered gridbased densitybased clique developed agrawal et al considered densitybased grid based clustering methods automatically finds subspaces high dimensional data space allow better clustering original space accuracy clustering result may degraded expense simplicity method clique evolutionary approaches based clustering methods famous evolutionary approaches include evolution strategies es evolutionary programming ep genetic algorithm ga particle swarm optimization pso ant colony optimization aco etc common approach evolutionary techniques data clustering follows approaches ga frequently used clustering solutions form binary strings gas selection operator propagates solutions current generation next generation based fitness selection employs probabilistic scheme choose random population solutions solution corresponds valid k partitions data associate fitness value solution typically fitness inversely proportional squared error value higher error smaller fitness vice versa solution small squared error larger fitness value use evolutionary operators viz selection recombination mutation generate next population solutions evaluate fitness values solutions repeat step termination condition satisfied solutions higher fitness higher probability getting reproduced major problem gas sensitivity selection various parameters population size crossover mutation probabilities etc grefenstette studied problem suggested guidelines selecting control parameters general steps ga clustering search based clustering approaches search techniques basically used obtain optimum value minimum maximum criterion function eg distance called objective function also search based approaches categorized stochastic deterministic search techniques stochastic search techniques evolve approximate optimal solution based fitness value stochastic techniques evolutionary approaches based rest search techniques come deterministic search techniques guarantee optimal solution performing exhaustive enumeration deterministic approaches typically greedy descent approaches stochastic search techniques either sequential parallel simulated annealing sa evolutionary approaches inherently parallel simulated annealing procedures designed avoid recover solutions correspond local optima objective functions accomplished accepting probability new solution next iteration lower quality measured criterion function probability acceptance governed critical parameter called temperature analogy annealing metals typically specified terms starting first iteration final temperature value al sultan et al studied effects control parameters performance algorithm used sa obtain near optimal partition data sa statistically guaranteed find global optimal solution sa algorithm slow reaching optimal solution optimal results require temperature decreased slowly iteration iteration tabu search like sa method designed cross boundaries feasibility local optimality systematically impose release constraints permit exploration otherwise forbidden regions tabu search used solve clustering problem collaborative fuzzy clustering relatively recent type clustering various applications database distributed several sites collaborative clustering proposed pedrycz concerns process revealing structure common similar number subsets mainly two forms collaborative clustering horizontal vertical collaborative clustering horizontal collaborative clustering database split different subsets features subset patterns database horizontal collaborative clustering applied mamdani type fuzzy inference system order decide association datasets vertical collaborative clustering database divided subsets patterns pattern subset features input instance set k number clusters n population size output clusters randomly create population n structures corresponds valid kclusters data repeat associate fitness value structure population b regenerate new generation structures termination condition satisfied objective function horizontal collaboration technique explained eq vertical collaboration technique please refer p n c n n ij ij ij ij ij j j l q l u l l l u l u l user defined parameter based datasets l denotes collaborative coefficient collaborative effect dataset l c number cluster l p p number datasets n number patterns dataset u represents partition matrix n number features euclidean distance patterns prototypes general scheme collaborative clustering shown fig demonstrates connections matrices order accomplish collaboration subsets dataset first solve problem dataset separately allow results interact globally forming collaborative process datasets collaborative fuzzy partitioning carried iterative optimization objective function shown eq optimization ql involves determination partition matrix u prototypes v different data sets shown fig b collaborative clustering scheme two datasets b collaborative clustering scheme three datasets fig collaborative clustering scheme multi objective clustering case multiobjective clustering many clustering approaches optimized simultaneously multiobjective clustering automatic kdetermination mock compactness clusters maximized first objective connectivity clusters maximized second objective pareto approach used optimize aforesaid two objectives simultaneously multi objective clustering ensemble moce proposed faceili etal uses mock along special crossover operator utilizes ensemble clustering law et al different clustering methods different objectives used surveys seen overlapping clustering overlapping community detection partition clustering usually indicates exclusive overlapping clustering algorithms like k means discussed member object belongs one cluster object belongs one cluster becomes overlapping clustering method algorithm eg fuzzy cmeans clustering nowadays community detection effective way reveal relationship structure function networks drawn lots attention well developed networks modeled graphs nodes represent objects edges represent interactions among community detection divides network groups nodes nodes densely connected inside sparsely connected outside however real world objects often diverse roles belong multiple communities example professor collaborates researchers different fields person family group well friend group time community detection objects divided multiple groups known overlapping nodes aim overlapping community detection discover overlapping nodes communities lots overlapping community detection approaches proposed roughly divided two categories nodebased linkbased algorithms nodebased overlapping community detection algorithms directly divide nodes network different communities based intuition link networks usually represents unique relation linkbased algorithms firstly cluster edges network map link communities node communities gathering nodes incident edges within link community newly proposed linkbased algorithms shown superiority detecting complex multiscale communities however high computational complexities bias discovered communities shi et al proposed genetic algorithm gaocd overlapping community detection based link clustering framework different node based overlapping community detection algorithms gaocd utilized property unique role links applies novel genetic algorithm cluster edges experiments artificial real networks showed gaocd effectively reveal overlapping structure evaluation criteria formation clusters important process however also meaningful test validity accuracy clusters formed method tested whether clusters formed certain method show maximum similarity among objects cluster minimum similarity among clusters recently many evaluation criteria developed criteria divided mainly two categories internal external internal quality criteria measures internal criteria generally measure compactness clusters applying similarity measure techniques general measures intercluster separability intracluster homogeneity combination two sum squared error sum square error sse frequently used criterion measure clustering defined k k k k x c sse x ck set instances cluster k k vector mean cluster k scatter criteria scatter criteria matrix defined follows kth cluster k k k k x c x x condorcets criterion condorcets criterion another approach apply ranking problem criterion defined follows j k j k j k j k j k c c x x c c c x c x c x x x x x x sxj xk dxj xk measure similarity distance vectors xj xk ccriterion fortier solomon defined ccriterion extension condorcets criterion defined j k j k j k j k j k c c x x c c c x c x c x x x x x x threshold value category utility metric category utility defined measures goodness category set entities size n binary feature set f fi n binary category c c c calculated follows log log log n n n cu c f p c p f c p f c p c p f c p f c p f p f pc prior probability entity belonging positive category c p f c conditional probability entity feature fi given entity belongs category c p f c likewise conditional probability entity feature fi given entity belongs category c pfi prior probability entity processing feature fi edge cut metrics edge cut minimization problem useful cases dealing clustering problems case cluster quality measured ratio remaining edge weights total precut edge weights finding optimal value easy edge cut minimization problem restriction size clusters external quality criteria measures order match structure cluster predefined classification instances external quality criteria measure useful mutual information based measure strehl et al proposed mutual information based measure used external measure clustering criteria measure instances clustered using c ccg referring target attribute z whose domain domz cck defined follows log g k l h l h g k l h l l c mlh indicates number instances cluster cl also class ch mh denotes total number instances class ch similarly ml indicates number instances cluster cl rand index rand index simple criterion used compute similar clusters benchmark classifications rand index defined tp tn rand tp fp fn tn tp number true positives tn number true negatives fp number false positives fn number false negatives rand index lies two partitions agree perfectly rand index fmeasure rand index false positives false negatives equally weighted may cause undesirable features clustering applications fmeasure addresses concern used balance false negatives weighting recall parameter fmeasure defined follows p r f p r p precision rate r recall rate recall impact increasing allocates increasing amount weight recall final fmeasure precision recall defined follows tp p tp fp tp r tp fn jaccard index jaccard index considered identify equivalency two datasets jaccard index defined follows b tp j b b tp fp fn b empty j b ie j b simply number unique elements common sets divided total number unique elements sets fowlkesmallows index fowlkesmallows index determines similarity clusters obtained clustering algorithm higher value fowlkesmallows index indicates similarity clusters determined follows tp tp fm tp fp tp fn confusion matrix confusion matrix also known contingency table error matrix used quickly visualize results clustering classification system trained distinguish apples oranges tomatoes confusion matrix summarize results testing algorithm inspection assuming sample fruits apples oranges tomatoes result confusion matrix look like table table confusion matrix actual class predicted class apple orange tomato apple orange tomato external indices based prespecified structure reflection prior information data used standard validate clustering solutions internal tests dependent external information prior knowledge contrary examine clustering structure directly original data evaluation refer applications clustering useful several applications endless useful applications applications given diverse fields image segmentation image segmentation essential component image processing image segmentation achieved using hierarchical clustering kmeans also applied segmentation magnetic resonance imaging mri provides visualization internal structures objects living organisms mri images better contrast computerized tomography therefore medical image segmentation research uses mri images segmenting mri image key task many medical applications surgical planning abnormality detection mri segmentation aims partition input image significant anatomical areas uniform according certain image properties mri segmentation formulated clustering problem set feature vectors obtained transformation image measurements pixel positions grouped number structures bioinformaticsgene expression data recently advances genome sequencing projects dna microarray technologies achieved first draft human genome sequence project completed several years earlier expected applications clustering algorithms bioinformatics seen two aspects first aspect based analysis gene expression data generated dna microarray technologies second aspect describes clustering processes directly work linear dna protein sequences assumption functionally similar genes proteins usually share similar patterns primary sequence structures object recognition use clustering group views objects purposes object recognition range data described system consideration employed view point dependent view cantered approach object recognition problem object recognized represented terms library range images object character recognition clustering employed jain identify lexemes handwritten text purposes writer independent hand writing recognition success handwriting recognition system vitally dependent acceptance potential users writer dependent systems give higher level recognition accuracy given writer independent systems former require large amount training data writer independent system hand must able recognize wide variety writing styles order satisfy individual user information retrieval information retrieval ir concerned automatic storage retrieval documents many university libraries use ir systems provide access books journals documents libraries use library congress classification lcc scheme efficient storage retrieval books lcc scheme consists classes labelled z used characterize books belonging different subjects example label q corresponds books area science subclass qa assigned mathematics labels qa qa used classifying books related computers areas computer science data mining data mining extraction knowledge large databases applied relational transaction spatial databases well large stores unstructured data world wide web many data mining systems use today applications include us treasury detecting money laundering national basketball association coaches detecting trends patterns play individual players teams categorizing patterns children foster care system several articles recent published special issues data mining spatial data analysis clustering useful extract interesting features identify patterns exist huge amounts spatial databases expensive hard user deal large spatial datasets like satellite images medical equipment geographical information systems gis image database exploration etc clustering process helps understand spatial data analyzing process automatically business role clustering quite interesting business areas helps marketer researchers analysis prediction customers order provide services based requirements also helps market segmentation new product development product positioning clustering may used set available shopping items web group unique products data reduction data reduction compression one necessary tasks handling large data processing becomes demanding clustering applied help compressing data information clustering different set interesting clusters different set clusters choose information set data useful us process save data processing time along data reduction big data mining big data also emerging issue volume data beyond capacity conventional data base management tools processed big data mining due use various social sites travel egovernance etc practices mammoth amount data heaped every moment clustering information data help aggregating similar information collected unformatted databases mainly text hadoop one big data processing tool expected big data processing play important role detection cyber crime clustering groups people similar behaviour social network face book whatsapp etc predicting market behaviour based various polls social sites applications sequence analysis human genetic clustering social network analysis search result grouping software evolution recommender systems educational data mining climatology field robotics etc choice appropriate clustering methods depicted fig wide amount literature available referred paper becomes obvious question method uniformly good remember according free lunch concept given wolpert algorithm uniformly good circumstances fact algorithm merit strength specific nature data fails type data selection appropriate clustering method may sometimes also involve decision certain parameters whether one wants proper alignment unsupervised grouping objects number clusters say user define k choosing value k matters choice made fine tuning among intracluster objects patterns virtue distance expected selecting k heuristic stochastic evolutionary computing like genetic algorithms ga applied find k hand case data mining data processing applications dimensionality reduction mostly required reduce number attributes features existing dataset order extract rules better prediction capability many occasions expected reducing dimensionality dataset whether structure internal topology dataset disturbed reduced data space saxena et al proposed four unsupervised methods feature selection using genetic algorithms fraley presents comprehensive discussion decide clustering method described clustering methodology based multivariate normal mixture models shown give much better performance existing methods approach limitations however first limitation computational methods hierarchical clustering storage time requirements grow faster linear rate relative size initial partition directly applied large data sets secondly although experience date suggests models based multivariate normal distribution sufficiently flexible accommodate many practical situations underlying assumption groups concentrated locally linear subspaces models methods may suitable instances bensmail et al showed exact bayesian inference via gibbs sampling calculations bayes factors using laplacemetropolis estimator works well several real simulated examples large data sets cure method advisable whereas birch also good less time complexity although quality clustering inferior obtained cure refer table partitioned clustering method kmeans clustering dominates still popular clustering method refer table many clusters ie k depends close fine tuning want among clusters also keep mind purpose applying kmeans various clustering methods presented paper already strengths weaknesses mostly given therein apart discussion selection appropriate method clustering worth noting looking huge amount literature available wide variety application clustering possible settle agreeable recommendation specific task objectives calls specific strategy tested experimentally finally part comprehensive comparative table various clustering algorithms presented given table details meaning symbols refer table comparative study clustering algorithms category clustering algorithm name time complexity scalability suitable large scale data suitable high dimensional data sensitive noise outlier partition kmeans low oknt middle yes high pam high oknk low little clara middle oks knk high yes little clarans high middle yes little hierarchy birch low high yes little cure low os logs high yes yes little rock high onlogn middle yes little chameleon high high little fuzzy based fcm low middle high density based dbscan middle onlogn middle yes little graph theory click low okfve high yes high grid based clique low onk high yes moderate conclusions classification objects finds prime importance several data processing applications including data mining medical diagnostics pattern recognition social paradigms objects already labeled placed supervised classified groups labeled grouped unsupervised classified groups paper presented various methods used clusters states arts limitations hierarchical type clustering methods clusters formed iteratively dividing patterns instances topdown bottom manner accordingly agglomerative divisive splitting hierarchical clustering methods discussed opposed hierarchical clustering partitional clustering assigns data k clusters without hierarchical structure optimizing criterion function common criterion finding euclidean distance points available clusters assigning point cluster minimum distance benchmark kmeans clustering methods variations like fuzzy k means discussed graph theoretic methods produce clusters via graphs mixture density based methods data objects assumed generated according several probability distributions derived different types density functions eg multivariate gaussian distribution families different parameters grid based clustering techniques include sting statistical information grid approach highly scalable algorithm ability decompose data set various levels details evolutionary approaches clustering start random population candidate solutions fitness function would optimized clustering based simulated annealing collaborative clustering multi objective clustering states art also included various types similarity criteria clustering given paper clusters formed evaluation criteria also summarised see performance accuracy clusters applications clustering image segmentation object character recognition information retrieval data mining highlighted paper course abundant amount literature available clustering applications possible cover entirely basic important methods included paper merits demerits acknowledgement authors would like thank anonymous reviewers valuable suggestions comments improve quality paper work partially supported australian research council arc discovery grant dp references r duda p e hart g stork pattern classification wiley publications zhang yin guo x yu l xiao crossvalidation based weights structure determination chebyshevpolynomial neural networks pattern classication pattern recognition vol pp h nakayama n kagaku pattern classication linear goal programming extensions journal global optimization vol pp c bishop pattern recognition machine learning berlin springer isbn gp zhang neural networks classication survey ieee transaction systems man cybernetics part c applications reviews vol pp h zhang j liu z wang datacorebased fuzzy minmax neural network pattern classication ieee transaction neural networks vol pp x jiang h k wah constructing training feedforward neural net works pattern classication pattern recognition vol pp g ou l murphey multiclass pattern classication using neural networks pattern recognition vol pp j paola r schowengerdt detailed comparison back propagation neural network maximum likelihood classiers urban land use classication ieee transaction geoscience remote sensing vol pp e rumelhart j l mcclelland parallel distributed processing mit press cambridge w zhou verication nonparametric characteristics backpropagation neural networks image classication ieee transaction geoscience remote sensing vol pp g jaeger u c benz supervised fuzzy classification sar data using multiple sources ieee international geoscience remote sensing symposium f marzano scaranari g vulpiani supervised fuzzylogic classification hydrometeors using c band weather radars ieee transaction geoscience remote sensing vol pp b xue zhang w n browne particle swarm optimization feature selection classification multiobjective approach ieee transaction cybernetics vol pp saxena vora novel approach use small world theory particle swarm optimization th international conference advanced computing communications z pawlak rough sets international journal computer information science vol pp z pawlak rough sets theoretical aspects reasoning data kluwer netherlands dalai b chatterjee dey chakravorti k bhattacharya roughsetbased feature selection classification power quality sensing device employing correlation techniques ieee sensors journal vol pp j r quinlan induction decision trees machine learning vol pp farida l zhang c rahman hossain r strachan hybrid decision tree nave bayes classiers multiclass classication tasks expert systems applications vol pp j han kamber j pei data mining concepts techniques morgan kaufmann publishers l rokach clustering methods data mining knowledge discovery handbook pp springer saxena n r pal vora evolutionary methods unsupervised feature selection using sammons stress function fuzzy information engineering vol pp k jain data clustering years beyond kmeans pattern recognition letters vol pp merriamwebster online dictionary v e castro j yang fast robust general purpose clustering algorithm international conference articial intelligence c fraley e raftery many clusters clustering method answers via modelbased cluster analysis technical report department statistics university washington k jain n murty p j flynn data clustering review acm computing surveys vol pp p sneath r sokal numerical taxonomy wh freeman co san francisco ca b king stepwise clustering procedures journal american statistical association vol pp j h ward hierarchical grouping optimize objective function journal american statistical association vol pp f murtagh survey recent advances hierarchical clustering algorithms use cluster centers computer journal vol pp nagpal jatain gaur review based data clustering algorithms ieee conference information communication technologies periklis data clustering techniques university toronto guha r rastogi kyuseok cure efficient clustering algorithm large databases acm k george e h han v kumar chameleon hierarchical clustering algorithm using dynamic modeling ieee computer vol pp lam c wunsch clustering academic press library signal processing signal processing theory machine learning vol j b macqueen methods classification analysis multivariate observations th symposium mathematical statistics probability berkeley university california press vol pp gersho r gray vector quantization signal compression kluwer academic publishers j c dunn fuzzy relative isodata process use detecting compact wellseparated clusters journal cybernetics vol pp j c bezdek pattern recognition fuzzy objective function algorithms plenum press new york r yager filev approximate clustering via mountain method ieee transaction systems man cybernetics part b cybernetics vol pp gath geva unsupervised optimal fuzzy clustering ieee transaction pattern analysis machine intelligence vol pp r hathaway j bezdek hu generalized fuzzy cmeans clustering strategies using lp norm distances ieee transaction fuzzy systems vol pp r krishnapuram j keller possibilistic approach clustering ieee transaction fuzzy systems vol pp c zahn graphtheoretical methods detecting describing gestalt clusters ieee transaction computer vol c pp r urquhart graphtheoretical clustering based limited neighborhood sets pattern recognition vol pp h fisher knowledge acquisition via incremental conceptual clustering machine learning pp haykin neural networks comprehensive foundation nd edition prentice hall r xu wunsch survey clustering algorithms ieee transaction neural networks vol r xu dc wunsch clustering algorithms biomedical research review ieee reviews biomedical engineering vol pp g mclachlan krishnan em algorithm extensions wiley new york j baneld e raftery modelbased gaussian nongaussian clustering biometrics vol pp ester h p kriegel sander x xu densitybased algorithm discovering clusters large spatial databases noise nd international conference knowledge discovery data mining p cheeseman j stutz bayesian classication autoclass theory results advances knowledge discovery data mining pp c wallace l dowe intrinsic classication mmlthe snob program th australian joint conference articial intelligence pp w wang j yang r r muntz sting statistical information grid approach spatial data mining rd vldb conference pp g sheikholeslami chatterjee zhang wavecluster waveletbased clustering approach spatial data large databases international journal large data bases vol pp r agrawal g johannes g dimitrios p raghavan automatic subspace clustering high dimensional data data mining applications sigmod conference pp k jain flynn data clustering review acm computing surveys csur vol pp h p schwefel numerical optimization computer models john wiley new york l j fogel j owens j walsh artificial intelligence simulated evolution john wiley new york j h holland adaption natural artificial systems university michigan press goldberg genetic algorithms search optimization machine learning addison wesley reading j kennedy r c eberhart swarm intelligence morgan kaufmann j kennedy r eberhart particle swarm optimization th ieee international conference neural networks pp dorigoand sttzle ant colony optimization mit press f glover future paths integer programming links artificial intelligence computers operations research vol pp k al sultan tabu search approach clustering problem pattern recognition vol pp w pedrycz collaborative fuzzy clustering pattern recognition letters vol pp l f coletta l vendramin e r hruschka r j g b campello w pedrycz collaborative fuzzy clustering algorithms renements design guidelines ieee transactions fuzzy systems vol pp w pedrycz p rai collaborative clustering use fuzzy cmeans quantication fuzzy sets systems vol pp w pedrycz knowledge based clustering data information granules wiley publications prasad c lin c yang saxena vertical collaborative fuzzy cmeans multiple eeg data sets springer intelligent robotics applications lecture notes computer science vol pp c pizzuti overlapping community detection complex networks gecco pp gregory fast algorithm find overlapping communities networks pkdd pp ahn j p bagrow lehmann link communities reveal multiscale complexity networks nature vol pp g forestier p gancarski c wemmert collaborative clustering back ground knowledge data knowledge engineering vol pp j handl j knowles evolutionary approach multiobjective clustering ieee transaction evolutionary computation vol pp konak coit smith multiobjective optimization using genetic algorithms tutorial reliability engineering system safety vol pp k faceili carvalho souto multiobjective clustering ensemble international conference hybrid intelligent systems k law topchy k jain multiobjective data clustering ieee conference computer vision pattern recognition vol pp forsyth j ponce computer vision modern approach prentice hall h g consortium initial sequencing analysis human genome nature vol pp c dorai k jain shape spectra based view grouping free form object international conference image processing vol pp connell k jain learning prototypes online handwritten digits th international conference pattern recognition vol pp e rasmussen clustering algorithms information retrieval data structures algorithms prentice hall englewood cliffs pp g mckiernan lc classification outline library congress washington c r hedberg searching mother lode tales first data miners ieee expert intelligent systems applications vol pp j cohen communications acm data mining association computing machinery nov saxena j wang dimensionality reduction unsupervised feature selection applying non euclidean norms classification accuracy international journal data warehousing mining vol pp k al sultan khan computational experience four algorithms hard clustering problem pattern recognition letters vol pp r michalski r e stepp e diday automated construction classifications conceptual clustering versus numerical taxonomy ieee transaction pattern analysis machine intelligence vol pp j c venter et althe sequence human genomesciencevol pp j l kolodner reconstructive memory computer model cognitive science vol pp c carpineto g romano ordertheoretic approach conceptual clustering th international conference machine learning pp l talavera j bejar generalitybased conceptual clustering probabilistic concepts ieee transactions pattern analysis machine intelligence vol pp hadzikadic yun concept formation incremental conceptual clustering th international joint conference artificial intelligence pp g biswas j b weinberg h fisher iterate conceptual clustering algorithm data mining ieee transactions systems man cybernetics part c vol pp k thompson p langley concept formation structured domains concept formation knowledge experience unsupervised learning morgan kaufmann jonyer cook l holder graphbased hierarchical conceptual clustering journal machine learning research vol pp lebowitz experiments incremental concept formation unimem machine learning vol pp hanson bauer conceptual clustering categorization polymorphy machine learning journal vol pp kohonen selforganizing map neurocomputing vol pages j vesanto e alhoniemi clustering selforganizing map ieee transactions neural networks vol j g upton b fingelton spatial data analysis example point pattern quantitative data john wiley sons new york vol strehl j ghosh r mooney impact similarity measures webpage clustering workshop artificial intelligence web search pp j j fortier h solomon clustering procedures multivariate analysis pp gluck j e corter information uncertainty utility categories program th annual conference cognitive science society pp j n condorcet essai sur lapplication de lanalyse la probabilite des decisions rendues la pluralite des voix paris limprimerie royale j f marcotorchino p michaud optimisation en analyse ordinale des donnees masson paris j e corter gluck explaining basic categories feature predictability information psychological bulletin vol pp strehl j ghosh clustering guidance quality evaluation using relationshipbased visualization intelligent engineering systems artificial neural networks st louis missouri usa pp v stehman selecting interpreting measures thematic classification accuracy remote sensing environment vol pp w rand objective criteria evaluation clustering methods journal american statistical association vol pp v rijsbergen information retrieval butterworths london j f brendan dueck clustering passing messages data pointsscience vol pp e b fowlkes c l mallows method comparing two hierarchical clusterings journal american statistical association vol pp l olson delen advanced data mining techniques springer st edition w powers evaluation precision recall ffactor roc informedness markedness correlation journal machine learning technologies vol pp p jaccard distribution de la flore alpine dans le bassin des dranses et dans quelques rgions voisines bulletin de la socit vaudoise des sciences naturelles vol pp j han kamber j pei data mining concepts techniques morgan kaufman san francisco usa j j grefenstette optimization control parameters genetic algorithms ieee transaction systems man cybernetics vol pp c lin prasad j chang designing mamdani type fuzzy rule using collaborative fcm scheme international conference fuzzy theory applications l eugene chapter combinatorial implications maxflow mincut theorem chapter linear programming interpretation maxflow mincut theorem combinatorial optimization networks matroids dover pp c h papadimitriou k steiglitz chapter maxflow mincut theorem combinatorial optimization algorithms complexity dover pp fotheringham e charlton c brunsdon geographically weighted regression natural evolution expansion method spatial data analysis environment planning vol pp honarkhah j caers stochastic simulation patterns using distancebased pattern modeling mathematical geosciences vol pp p tahmasebi hezarkhani sahimi multiplepoint geostatistical modeling based cross correlation functions computational geosciences vol pp guha r rastogi k shim rock robust clustering algorithm categorical attributes ieee conference data engineering zhang r ramakrishnan linvy birch efficient method large databases acm sigmod jiang g chen b c ooi k l tan w epic extensible scalable system processing big data th vldb conference pp z huang fast clustering algorithm cluster large categorical data sets data mining dmkd hinneburg keim efficient approach clustering large multimedia databases noise kdd conference j berry g linoff data mining techniques marketing sales customer support john wiley sons inc usa g fennell g allenby yang edwards effectiveness demographics phychographic variables explaining brand product category use quantitative marketing economics vol pp kiang fisher hu effect sample size extended selforganizing map network market segmentation application computational statistics data analysis vol pp dolnicar using cluster analysis market segmentationtypical misconceptions established methodological weaknesses recommendations improvement journal marketing research vol pp r wagner w scholz r decker number clusters market segmentation data analysis decision support heidelberg springer pp r durbin r eddy krogh g mitchison biological sequence analysis probabilistic models proteins nucleic acids cambridge cambridge university press j kaplanel r g winther prisoners abstraction theory measure genetic variation concept race biological theory vol p j carrington j scott social network analysis introduction sage handbook social network analysis london vol yippy growing leaps bounds newspress may retrieved may dirk conceptoriented approach support software maintenance reuse activities th joint conference knowledge based software engineering g b dias n anquetil k oliveira organizing knowledge used software maintenance journal universal computer science vol pp r francesco l rokach b shapira introduction recommender systems handbook recommender systems handbook springer pp wwweducationaldataminingorg r baker data mining education international encyclopedia education rd edition oxford uk elsevier vol pp g siemens r j baker learning analytics educational data mining towards communication collaboration nd international conference learning analytics knowledge pp r huth c beck philipp demuzere z ustrnul cahynova j kysely e tveito classifications atmospheric circulation patterns recent advances applications annals new york academy science vol pp bewley r shekhar leonard b upcroft p lever realtime volume estimation dragline payload ieee international conference robotics automation pp c manning p raghavan h schutze introduction information retrieval cambridge university press nguyen l chen c k chan clustering multiviewpointbased similarity measure ieee transactions knowledge data engineering vol pp bravais memoires par divers savants ix paris pp k pearson mathematical contributions theory evolution iii regression heredity panmixia philosophical transactions royal society london series vol pp srensen method establishing groups equal amplitude plant sociology based similarity species application analyses vegetation danish commons kongelige danske videnskabernes selskab vol pp l r dice measures amount ecologic association species ecology vol pp j hamilton time series analysis princeton university press r tsay analysis financial time series john wiley sons saxena j wang dimensionality reduction unsupervised feature selection applying non euclidean norms classification accuracy international journal data warehousing mining ijdwm vol pp arora chana survey clustering techniques big data analysis th international conference next generation information technology summit confluence shirkhorshidi aghabozorgi wah herawan big data clustering review lecture notes computer science vol pp h wang w wang j yang p yu clustering pattern similarity large data sets international conference management data acm z huang fast clustering algorithm cluster large categorical data sets data mining dmkd x wu x zhu g q wu w ding data mining big data ieee transaction knowledge data engineering vol pp p russom big data analytics tdwi best practices report fourth quarter c xiao f nie h huang multiview kmeans clustering big data twentythird international joint conference artificial intelligence aaai w fan b albert mining big data current status forecast future acm sigkdd explorations newsletter vol pp k shvachko h kuang radia r chansler hadoop distributed file system ieee th symposium mass storage systems technologies msst jeffrey ghemawat mapreduce flexible data processing tool communications acm vol pp httpshadoopapacheorg g celeux g govaert classification em algorithm clustering two stochastic versions computational statistics data analysis vol pp l kaufman p rousseeuw finding groups data introduction cluster analysis wiley r ngand j han clarans method clustering objects spatial data mining ieee trans knowledge data engineering vol pp sisodia singh sisodia saxena clustering techniques brief survey different clustering algorithms international journal latest trends engineering technology ijltet vol pp zhong miao wang graphtheoretical clustering method based two rounds minimum spanning trees pattern recognition vol pp chen sanghavi h xu improved graph clustering ieee transactions information theory vol pp condon r karp algorithms graph partitioning planted partition model random structures algorithms vol pp w e donath j hoffman lower bounds partitioning graphs ibm j res develop vol pp j shi j j malik normalized cuts image segmentation ieee transactions pattern analysis machine intelligence vol pp u luxburg tutorial spectral clustering statistics computing vol pp k rohe chatterjee b yu spectral clustering highdimensional stochastic block model annals statistics vol pp gunnemann farber b boden seidl subspace clustering meets dense subgraph mining synthesis two paradigms icdm k macropol singh scalable discovery best clusters large graphs proceedings vldb endowment vol pp j j whang x sui dhillon scalable memoryefficient clustering largescale social networks icdm g karypis v kumar fast high quality multilevel scheme partitioning irregular graphs siam journal scientific computing vol pp g karypis v kumar multilevel kway partitioning scheme irregular graphs journal parallel distributed computing vol pp yan l huang jordan fast approximate spectral clustering kdd pp j liu c wang danilevsky j han largescale spectral clustering graphs ijcai w yang h xu divide conquer framework distributed graph clustering icml ghosh dubey comparative analysis kmeans fuzzy c means algorithms international journal advanced computer science applications vol pp niwattanakul j singthongchai e naenudorn wanapu using jaccard coefficient keywords similarity proceedings international multiconference engineers computer scientists vol imecs march hong kong c chen l pau p wang hand book pattern recognition computer vision eds world scientific singapore pp rdubes cluster analysis related issue jain r dubes algorithms clustering data englewood cliffs nj prenticehall c shi cai fu dong b wu link clustering based overlapping community detection algorithm data knowledge engineering vol pp g palla derenyi farkas vicsek uncovering overlapping community structure complex networks nature society nature vol pp h wolpert w g macready free lunch theorem optimization ieee transactions evolutionary computation vol pp bensmail h celeux g raftery e robert c p inference modelbased cluster analysis statcomput xud tian comprehensive survey f clustering algorithms ann data sci'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df['processed_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff132a25-6766-4fe4-8466-5fdd33b1d2ca",
   "metadata": {},
   "source": [
    "## Method A: TF-IDF with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab0166f4-becd-4a04-ad28-ac87a7a70730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF matrix...\n",
      "TF-IDF matrix created with shape: (637, 61275)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 1. Create the TF-IDF Matrix from your database ---\n",
    "print(\"Creating TF-IDF matrix...\")\n",
    "\n",
    "# We use ngrams to capture 2-word phrases (e.g., \"machine learning\")\n",
    "# min_df=5: ignore terms that appear in fewer than 5 papers\n",
    "# max_df=0.8: ignore common terms that appear in > 80% of papers\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5, ngram_range=(1,2))\n",
    "\n",
    "# This creates a sparse matrix of TF-IDF features for all 637 papers\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(author_df['processed_text'])\n",
    "\n",
    "print(\"TF-IDF matrix created with shape:\", tfidf_matrix.shape)\n",
    "\n",
    "\n",
    "# --- 2. Create the Recommendation Function ---\n",
    "def recommend_with_tfidf(input_pdf_path, k=5):\n",
    "    \"\"\"Recommends top k reviewers for an input PDF using TF-IDF.\"\"\"\n",
    "    \n",
    "    # a. Process the input paper\n",
    "    input_text = extract_text_from_pdf(input_pdf_path)\n",
    "    processed_input = preprocess_text(input_text)\n",
    "    \n",
    "    # b. Transform the input paper into a TF-IDF vector\n",
    "    # Use .transform() NOT .fit_transform() - we use the vocabulary from our database\n",
    "    input_vector = tfidf_vectorizer.transform([processed_input])\n",
    "    \n",
    "    # c. Compute cosine similarity against all papers in the database\n",
    "    similarities = cosine_similarity(input_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # d. Add these scores to our DataFrame\n",
    "    author_df['similarity_score'] = similarities\n",
    "    \n",
    "    # e. Find the top-scoring paper for each author and rank the authors [cite: 43]\n",
    "    top_authors = author_df.groupby('author')['similarity_score'].max().sort_values(ascending=False)\n",
    "    \n",
    "    return top_authors.head(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38e5bf-3110-4d33-952a-ff62aca42448",
   "metadata": {},
   "source": [
    "## Method B: Sentence Transformers (Semantic Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c74e94-872b-4eb1-91cb-bf76f8a2cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence embedding model...\n",
      "Generating embeddings for all papers in the database... (This may take a few minutes)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4426addff0664ea9ad93f589e3401cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated with shape: (637, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load the Pre-trained Model ---\n",
    "# This will be downloaded on the first run. 'all-MiniLM-L6-v2' is a great, fast model.\n",
    "print(\"Loading sentence embedding model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# --- 2. Generate Embeddings for all papers in the database ---\n",
    "# This is a one-time, heavy computation. We are converting all 637 papers into vectors.\n",
    "print(\"Generating embeddings for all papers in the database... (This may take a few minutes)\")\n",
    "paper_embeddings = model.encode(author_df['processed_text'].tolist(), show_progress_bar=True)\n",
    "print(\"Embeddings generated with shape:\", paper_embeddings.shape)\n",
    "\n",
    "\n",
    "# --- 3. Create the Recommendation Function ---\n",
    "def recommend_with_embeddings(input_pdf_path, k=5):\n",
    "    \"\"\"Recommends top k reviewers for an input PDF using sentence embeddings.\"\"\"\n",
    "    \n",
    "    # a. Process the input paper\n",
    "    input_text = extract_text_from_pdf(input_pdf_path)\n",
    "    processed_input = preprocess_text(input_text)\n",
    "    \n",
    "    # b. Generate embedding for the single input paper\n",
    "    input_embedding = model.encode([processed_input])\n",
    "    \n",
    "    # c. Compute cosine similarity against all database embeddings\n",
    "    similarities = cosine_similarity(input_embedding, paper_embeddings).flatten()\n",
    "    \n",
    "    # d. Add these scores to our DataFrame\n",
    "    author_df['similarity_score'] = similarities\n",
    "    \n",
    "    # e. Find the top-scoring paper for each author and rank the authors\n",
    "    top_authors = author_df.groupby('author')['similarity_score'].max().sort_values(ascending=False)\n",
    "    \n",
    "    return top_authors.head(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a272a-ce7e-41c5-8002-2e6cd7aeee93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5801ea4c-6cb0-4a8b-975d-1bfe82f7e17e",
   "metadata": {},
   "source": [
    "## TOP K-Recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e9b15c-ae6c-4ac4-84ff-3363c19b1853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing with paper: Survey_of_E-Governance_Systems_with_focus_on_Devel.pdf ---\n",
      "Paper Author: Geeta Rani\n",
      "\n",
      "\n",
      "--- Top 5 Reviewers (TF-IDF - Keyword Match) ---\n",
      "author\n",
      "Geeta Rani           1.000000\n",
      "Shikha Gupta         0.164366\n",
      "Dr. Ashish Jain      0.161370\n",
      "Krishna Asawa        0.158877\n",
      "Bharghava Rajaram    0.132669\n",
      "Name: similarity_score, dtype: float64\n",
      "\n",
      "--- Top 5 Reviewers (Embeddings - Semantic Match) ---\n",
      "author\n",
      "Geeta Rani            1.000000\n",
      "Pinaki Chakraborty    0.739861\n",
      "Shikha Gupta          0.716990\n",
      "K.V. Sambasivarao     0.703460\n",
      "V. Ravi               0.682987\n",
      "Name: similarity_score, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "# --- Pick a Test Paper ---\n",
    "# Let's just grab the 10th paper in your database as an example.\n",
    "# (You can change 10 to any other number to test different papers)\n",
    "test_paper_index = 200\n",
    "INPUT_PAPER_PATH = author_df.iloc[test_paper_index]['full_path']\n",
    "print(f\"--- Testing with paper: {author_df.iloc[test_paper_index]['paper']} ---\")\n",
    "print(f\"Paper Author: {author_df.iloc[test_paper_index]['author']}\\n\")\n",
    "\n",
    "\n",
    "# --- Run Method A: TF-IDF ---\n",
    "top_reviewers_tfidf = recommend_with_tfidf(INPUT_PAPER_PATH, k=5)\n",
    "print(\"\\n--- Top 5 Reviewers (TF-IDF - Keyword Match) ---\")\n",
    "print(top_reviewers_tfidf)\n",
    "\n",
    "\n",
    "# --- Run Method B: Embeddings ---\n",
    "top_reviewers_embeddings = recommend_with_embeddings(INPUT_PAPER_PATH, k=5)\n",
    "print(\"\\n--- Top 5 Reviewers (Embeddings - Semantic Match) ---\")\n",
    "print(top_reviewers_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7225ff-e7af-451d-bd11-8e75799d0516",
   "metadata": {},
   "source": [
    "## Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f315a7e6-e29c-4459-97eb-11fa13071b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_methods(input_pdf_path, k=5):\n",
    "    \"\"\"\n",
    "    Runs both TF-IDF and Embeddings models on a single paper\n",
    "    and prints their recommendations side-by-side for comparison.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # --- Get Test Paper Info ---\n",
    "    # Find the row in our DataFrame matching the input path\n",
    "    test_paper_info = author_df[author_df['full_path'] == input_pdf_path].iloc[0]\n",
    "    print(f\"TESTING PAPER: {test_paper_info['paper']}\")\n",
    "    print(f\"ACTUAL AUTHOR: {test_paper_info['author']}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # --- 1. Run TF-IDF ---\n",
    "    tfidf_recs = recommend_with_tfidf(input_pdf_path, k)\n",
    "    \n",
    "    # --- 2. Run Embeddings ---\n",
    "    embedding_recs = recommend_with_embeddings(input_pdf_path, k)\n",
    "    \n",
    "    # --- 3. Format Results ---\n",
    "    # Create a DataFrame for easy viewing\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Rank': [i+1 for i in range(k)],\n",
    "        'TF-IDF Recommendation': tfidf_recs.index,\n",
    "        'TF-IDF Score': tfidf_recs.values,\n",
    "        'Embedding Recommendation': embedding_recs.index,\n",
    "        'Embedding Score': embedding_recs.values\n",
    "    })\n",
    "    \n",
    "    print(comparison_df.to_string()) # .to_string() prints it nicely\n",
    "    return comparison_df, test_paper_info['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3004a65-6b71-4310-a60b-d2f4347180b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING PAPER: A Novel Method for Summarization.pdf\n",
      "ACTUAL AUTHOR: Aruna Malapati\n",
      "============================================================\n",
      "   Rank    TF-IDF Recommendation  TF-IDF Score Embedding Recommendation  Embedding Score\n",
      "0     1           Aruna Malapati      1.000000           Aruna Malapati         1.000000\n",
      "1     2         Dr.Manpreet Kaur      0.281620            Krishna Asawa         0.761541\n",
      "2     3            Krishna Asawa      0.159655            Pabitra Mitra         0.740464\n",
      "3     4             Shikha Gupta      0.123185       Pinaki Chakraborty         0.728672\n",
      "4     5  Ramalinga Swamy Cheruku      0.122058         Dr.Manpreet Kaur         0.726751\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESTING PAPER: A Review on Swarm Intelligence Techniques.pdf\n",
      "ACTUAL AUTHOR: Dr. Ashish Jain\n",
      "============================================================\n",
      "   Rank   TF-IDF Recommendation  TF-IDF Score Embedding Recommendation  Embedding Score\n",
      "0     1         Dr. Ashish Jain      1.000000          Dr. Ashish Jain         1.000000\n",
      "1     2  Prakash Chandra Sharma      1.000000   Prakash Chandra Sharma         1.000000\n",
      "2     3        Dr. Shikha Mehta      0.263580             Shikha Gupta         0.667271\n",
      "3     4          Jagdish Bansal      0.228330           Jagdish Bansal         0.661170\n",
      "4     5              Geeta Rani      0.220509          Himanshu Mittal         0.616296\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESTING PAPER: Improved-Coverage Preserving Clustering Protocol in Wireless Sensor Network.pdf\n",
      "ACTUAL AUTHOR: Manju_JaypeeTech\n",
      "============================================================\n",
      "   Rank  TF-IDF Recommendation  TF-IDF Score Embedding Recommendation  Embedding Score\n",
      "0     1       Manju_JaypeeTech      1.000000         Manju_JaypeeTech         1.000000\n",
      "1     2  Rama Murthy Garimella      0.470525    Rama Murthy Garimella         0.777682\n",
      "2     3   Navneet Pratap Singh      0.409639     Navneet Pratap Singh         0.747837\n",
      "3     4        Dr.Ruchi Mittal      0.406890        K.V. Sambasivarao         0.712486\n",
      "4     5      Nishchal K. Verma      0.374232        Nishchal K. Verma         0.708187\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESTING PAPER: Real-Time Adaptive Control of a Flexible Manipulator.pdf\n",
      "ACTUAL AUTHOR: Prof. B Subudhi\n",
      "============================================================\n",
      "   Rank TF-IDF Recommendation  TF-IDF Score Embedding Recommendation  Embedding Score\n",
      "0     1       Prof. B Subudhi      1.000000          Prof. B Subudhi         1.000000\n",
      "1     2     Nishchal K. Verma      0.272312        Nishchal K. Verma         0.735427\n",
      "2     3         Tingwen Huang      0.218747            Tingwen Huang         0.705410\n",
      "3     4   Venkata Dilip Kumar      0.166628               Tandra Pal         0.631731\n",
      "4     5   Sreedhar Madichetty      0.139689             Yew-Soon Ong         0.578912\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "TESTING PAPER: Impact_on_structural_behavior_due_to_installation_.pdf\n",
      "ACTUAL AUTHOR: Venkata Dilip Kumar\n",
      "============================================================\n",
      "   Rank TF-IDF Recommendation  TF-IDF Score Embedding Recommendation  Embedding Score\n",
      "0     1   Venkata Dilip Kumar      1.000000      Venkata Dilip Kumar         1.000000\n",
      "1     2       Himanshu Mittal      0.167354        K.V. Sambasivarao         0.601200\n",
      "2     3       Prof. B Subudhi      0.127765               Geeta Rani         0.597623\n",
      "3     4    Pinaki Chakraborty      0.111498          Himanshu Mittal         0.586735\n",
      "4     5       Dr. Ashish Jain      0.096007     Navneet Pratap Singh         0.572911\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Select a few different test papers by their index ---\n",
    "test_indices = [50, 150, 300, 450, 600]\n",
    "\n",
    "# This list will hold our results for the final report\n",
    "evaluation_results = []\n",
    "\n",
    "for index in test_indices:\n",
    "    try:\n",
    "        INPUT_PAPER_PATH = author_df.iloc[index]['full_path']\n",
    "        result_df, actual_author = compare_methods(INPUT_PAPER_PATH, k=5)\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            \"test_paper\": author_df.iloc[index]['paper'],\n",
    "            \"actual_author\": actual_author,\n",
    "            \"results\": result_df\n",
    "        })\n",
    "        print(\"\\n\\n\") # Add space between reports\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not test paper at index {index}: {e}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d6850-9b1b-4901-8943-dc5748cc3d7a",
   "metadata": {},
   "source": [
    "## Reviewer-Reviewer Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154b4091-eeec-4775-981e-abd64e2e20d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Reviewer-Reviewer Similarity Analysis ---\n",
      "Creating author profile vectors...\n",
      "Created 71 unique author profile vectors.\n",
      "Example profile vector (head):\n",
      "                         0         1         2         3         4    \\\n",
      "author                                                                 \n",
      "Amit Saxena        -0.010342 -0.054523 -0.016319  0.009369  0.035323   \n",
      "Amita Jain          0.010456 -0.049848 -0.031692  0.000913 -0.002719   \n",
      "Animesh Chaturvedi -0.035180 -0.053205 -0.037727 -0.000533 -0.016629   \n",
      "Ankita Jain        -0.036092  0.000081 -0.020570 -0.048243  0.004191   \n",
      "Arun Chauhan       -0.032100 -0.066101 -0.009475  0.005755  0.021345   \n",
      "\n",
      "                         5         6         7         8         9    ...  \\\n",
      "author                                                                ...   \n",
      "Amit Saxena         0.007031 -0.036039  0.075279 -0.023437  0.019549  ...   \n",
      "Amita Jain         -0.018261 -0.067177  0.065371 -0.033055  0.006535  ...   \n",
      "Animesh Chaturvedi -0.049318 -0.008806  0.002975 -0.069638  0.012799  ...   \n",
      "Ankita Jain        -0.028456 -0.026820  0.030834 -0.074899  0.025301  ...   \n",
      "Arun Chauhan        0.038782 -0.014067 -0.002475 -0.028512 -0.021373  ...   \n",
      "\n",
      "                         374       375       376       377       378  \\\n",
      "author                                                                 \n",
      "Amit Saxena        -0.023746  0.063539 -0.035087  0.016708  0.069916   \n",
      "Amita Jain         -0.046522  0.093235  0.030583 -0.011498  0.070587   \n",
      "Animesh Chaturvedi  0.021447  0.037228  0.030002 -0.007815  0.084918   \n",
      "Ankita Jain         0.000928  0.066572  0.018399 -0.017207  0.040492   \n",
      "Arun Chauhan        0.017287  0.037251 -0.024897  0.045147  0.047996   \n",
      "\n",
      "                         379       380       381       382       383  \n",
      "author                                                                \n",
      "Amit Saxena        -0.038575  0.028121 -0.018530  0.023573  0.025208  \n",
      "Amita Jain          0.006595  0.094740 -0.034901  0.018308 -0.009625  \n",
      "Animesh Chaturvedi  0.047427  0.070239 -0.011827 -0.001919  0.000109  \n",
      "Ankita Jain         0.022217  0.089643 -0.025481 -0.069955 -0.006574  \n",
      "Arun Chauhan        0.019701  0.118937 -0.032974 -0.012461  0.018766  \n",
      "\n",
      "[5 rows x 384 columns]\n",
      "\n",
      "============================================================\n",
      "\n",
      "Calculating author-author similarity matrix...\n",
      "Author-Author Similarity Matrix (Top 5x5):\n",
      "author              Amit Saxena  Amita Jain  Animesh Chaturvedi  Ankita Jain  \\\n",
      "author                                                                         \n",
      "Amit Saxena            1.000000    0.699593            0.426722     0.418903   \n",
      "Amita Jain             0.699593    1.000000            0.498225     0.571534   \n",
      "Animesh Chaturvedi     0.426722    0.498225            1.000000     0.690595   \n",
      "Ankita Jain            0.418903    0.571534            0.690595     1.000000   \n",
      "Arun Chauhan           0.393750    0.444946            0.660242     0.613591   \n",
      "\n",
      "author              Arun Chauhan  \n",
      "author                            \n",
      "Amit Saxena             0.393750  \n",
      "Amita Jain              0.444946  \n",
      "Animesh Chaturvedi      0.660242  \n",
      "Ankita Jain             0.613591  \n",
      "Arun Chauhan            1.000000  \n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Top 5 Reviewers Similar to Aruna Malapati ---\n",
      "author\n",
      "Devendra K Tayal    0.904790\n",
      "Minni Jain          0.897086\n",
      "Dr.Manpreet Kaur    0.859262\n",
      "Krishna Asawa       0.848350\n",
      "Arun Chauhan        0.841331\n",
      "Name: Aruna Malapati, dtype: float32\n",
      "\n",
      "\n",
      "--- Top 5 Reviewers Similar to Dr. Ashish Jain ---\n",
      "author\n",
      "Shikha Gupta              0.821818\n",
      "Mukesh Prasad             0.802848\n",
      "Dr. Shikha Mehta          0.800786\n",
      "Nikhil Tripathi           0.789973\n",
      "Prakash Chandra Sharma    0.789221\n",
      "Name: Dr. Ashish Jain, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"--- Starting Reviewer-Reviewer Similarity Analysis ---\")\n",
    "\n",
    "# 1. Create an \"Author Profile\" Vector\n",
    "# We will average the embeddings of all papers for each author\n",
    "# to create a single vector (of 384 dimensions) that represents their total expertise.\n",
    "\n",
    "# We take the paper_embeddings (a numpy array) and give it the author names as an index\n",
    "print(\"Creating author profile vectors...\")\n",
    "author_vectors_df = (\n",
    "    pd.DataFrame(paper_embeddings)\n",
    "    .set_index(author_df['author'])\n",
    "    .groupby(level=0)  # Group by the index (author name)\n",
    "    .mean()            # Average all paper vectors for each author\n",
    ")\n",
    "\n",
    "print(f\"Created {len(author_vectors_df)} unique author profile vectors.\")\n",
    "print(\"Example profile vector (head):\")\n",
    "print(author_vectors_df.head())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# 2. Calculate the Similarity Matrix\n",
    "# Now we compare every author's profile vector against every other author's.\n",
    "print(\"Calculating author-author similarity matrix...\")\n",
    "author_similarity_matrix = cosine_similarity(author_vectors_df)\n",
    "\n",
    "# Let's put it in a nice DataFrame for easy reading\n",
    "# The index and columns will both be the list of author names\n",
    "author_sim_df = pd.DataFrame(\n",
    "    author_similarity_matrix,\n",
    "    index=author_vectors_df.index,\n",
    "    columns=author_vectors_df.index\n",
    ")\n",
    "\n",
    "print(\"Author-Author Similarity Matrix (Top 5x5):\")\n",
    "print(author_sim_df.iloc[:5, :5])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# 3. Create the \"Find Similar Reviewers\" Function\n",
    "def get_similar_reviewers(author_name, k=5):\n",
    "    \"\"\"Finds the top k most similar reviewers to a given reviewer.\"\"\"\n",
    "    \n",
    "    if author_name not in author_sim_df:\n",
    "        return f\"Error: Author '{author_name}' not found in the database.\"\n",
    "    \n",
    "    # Get the specified author's column (or row) of similarity scores\n",
    "    similar_scores = author_sim_df[author_name]\n",
    "    \n",
    "    # Sort the scores from highest to lowest\n",
    "    # and drop the first one (which is the author themselves, with a score of 1.0)\n",
    "    top_similar = similar_scores.sort_values(ascending=False).drop(author_name)\n",
    "    \n",
    "    # Return the top k\n",
    "    return top_similar.head(k)\n",
    "\n",
    "# --- EXAMPLE USAGE ---\n",
    "# Let's test it with an author from your previous analysis\n",
    "test_author_1 = \"Aruna Malapati\"\n",
    "similar_to_aruna = get_similar_reviewers(test_author_1)\n",
    "\n",
    "print(f\"--- Top 5 Reviewers Similar to {test_author_1} ---\")\n",
    "print(similar_to_aruna)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- ANOTHER EXAMPLE ---\n",
    "test_author_2 = \"Dr. Ashish Jain\"\n",
    "similar_to_ashish = get_similar_reviewers(test_author_2)\n",
    "\n",
    "print(f\"--- Top 5 Reviewers Similar to {test_author_2} ---\")\n",
    "print(similar_to_ashish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbce7b2-55f5-4130-8352-a50fdabad4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
