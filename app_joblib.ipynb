{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101cf363-045c-487b-a00c-b323f2ca6efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\VAMSHI KRISHNA BABU\\AppData\\Roaming\\Python\\Python313\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "--- Starting One-Time Pre-processing ---\n",
      "Building author database... (This will take time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 14it [00:10,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: library error: FT_New_Memory_Face(Times-Bold): unknown file format\n",
      "\n",
      "MuPDF error: library error: FT_New_Memory_Face(Times-Bold): unknown file format\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning Folders: 72it [02:10,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database built with 637 papers.\n",
      "Building TF-IDF model...\n",
      "Saving TF-IDF models to disk...\n",
      "Building Embedding model... (This will also take time)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e696f607c74c9dad5e885951957a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Embeddings to disk...\n",
      "Saving author database to disk...\n",
      "\n",
      "--- ✅ PRE-PROCESSING COMPLETE! ---\n",
      "The following files have been created in your folder:\n",
      " - author_database.parquet\n",
      " - paper_embeddings.npy\n",
      " - tfidf_vectorizer.joblib\n",
      " - tfidf_matrix.joblib\n",
      "\n",
      "You are now ready to use the fast app.py script.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from tika import parser as tika_parser\n",
    "from tika import detector\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib  # For saving models\n",
    "\n",
    "print(\"--- Starting One-Time Pre-processing ---\")\n",
    "\n",
    "# --- 1. Set up all paths and functions (from your notebook) ---\n",
    "\n",
    "# (Make sure these paths are correct)\n",
    "tesseract_install_path = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_install_path\n",
    "poppler_install_path = r\"C:\\Users\\VAMSHI KRISHNA BABU\\poppler\\poppler-25.07.0\\Library\\bin\"\n",
    "DATASET_FOLDER = r\"C:\\Users\\VAMSHI KRISHNA BABU\\Applied AI A-2\\Dataset\"\n",
    "\n",
    "# (Copy your helper functions here)\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc: text += page.get_text()\n",
    "        if text.strip(): return text\n",
    "    except Exception: pass\n",
    "    try:\n",
    "        parsed = tika_parser.from_file(pdf_path)\n",
    "        text = parsed.get('content')\n",
    "        if text and text.strip(): return text.strip()\n",
    "    except Exception: pass\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path, poppler_path=poppler_install_path)\n",
    "        ocr_text = \"\"\n",
    "        for img in images: ocr_text += pytesseract.image_to_string(img, lang='eng')\n",
    "        if ocr_text.strip(): return ocr_text\n",
    "    except Exception: return \"\"\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# --- 2. Build the database (This is the slow part) ---\n",
    "print(\"Building author database... (This will take time)\")\n",
    "authors_data = []\n",
    "for root, dirs, files in tqdm(os.walk(DATASET_FOLDER), desc=\"Scanning Folders\"):\n",
    "    for paper_file in files:\n",
    "        if paper_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(root, paper_file)\n",
    "            relative_path = os.path.relpath(root, DATASET_FOLDER)\n",
    "            author_name = relative_path.split(os.path.sep)[0]\n",
    "            raw_text = extract_text_from_pdf(pdf_path)\n",
    "            if raw_text:\n",
    "                processed_text = preprocess_text(raw_text)\n",
    "                authors_data.append({\n",
    "                    'author': author_name,\n",
    "                    'paper': paper_file,\n",
    "                    'processed_text': processed_text\n",
    "                })\n",
    "author_df = pd.DataFrame(authors_data)\n",
    "print(f\"Database built with {len(author_df)} papers.\")\n",
    "\n",
    "# --- 3. Build and save the TF-IDF Model ---\n",
    "print(\"Building TF-IDF model...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=5, ngram_range=(1,2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(author_df['processed_text'])\n",
    "print(\"Saving TF-IDF models to disk...\")\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(tfidf_matrix, 'tfidf_matrix.joblib')\n",
    "\n",
    "# --- 4. Build and save the Embedding Model ---\n",
    "print(\"Building Embedding model... (This will also take time)\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "paper_embeddings = model.encode(author_df['processed_text'].tolist(), show_progress_bar=True)\n",
    "print(\"Saving Embeddings to disk...\")\n",
    "np.save('paper_embeddings.npy', paper_embeddings)\n",
    "\n",
    "# --- 5. Save the clean Author DataFrame ---\n",
    "# We use Parquet because it's fast and efficient\n",
    "print(\"Saving author database to disk...\")\n",
    "author_df.to_parquet('author_database.parquet')\n",
    "\n",
    "print(\"\\n--- ✅ PRE-PROCESSING COMPLETE! ---\")\n",
    "print(\"The following files have been created in your folder:\")\n",
    "print(\" - author_database.parquet\")\n",
    "print(\" - paper_embeddings.npy\")\n",
    "print(\" - tfidf_vectorizer.joblib\")\n",
    "print(\" - tfidf_matrix.joblib\")\n",
    "print(\"\\nYou are now ready to use the fast app.py script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8aea80-3ea4-4756-9cf7-ddc3f09f3ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
